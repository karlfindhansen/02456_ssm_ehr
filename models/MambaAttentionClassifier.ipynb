{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VigXREV0sdsK",
        "outputId": "d1d6ac1a-95bc-4acd-a5b3-01f336509f72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "project_dir = '/content/drive/My Drive/ssm_ehr'\n",
        "print(os.path.exists(project_dir))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hO-kAOqotHYo",
        "outputId": "425a9b2d-9b9d-4a56-e015-dd37dd22b0c4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip uninstall mamba-ssm causal-conv1d\n",
        "!pip install causal-conv1d && pip install mamba-ssm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OT70cV1KxKtC",
        "outputId": "1ba623de-e76a-4a44-f2ef-376bce2d7307"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch==2.4.0 in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision==0.19.0 in /usr/local/lib/python3.10/dist-packages (0.19.0+cu121)\n",
            "Requirement already satisfied: torchaudio==2.4.0 in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.19.0) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.19.0) (11.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0) (12.6.77)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.0) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.0) (1.3.0)\n",
            "\u001b[33mWARNING: Skipping mamba-ssm as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping causal-conv1d as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting causal-conv1d\n",
            "  Using cached causal_conv1d-1.4.0.tar.gz (9.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from causal-conv1d) (2.4.0+cu121)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from causal-conv1d) (24.2)\n",
            "Collecting ninja (from causal-conv1d)\n",
            "  Downloading ninja-1.11.1.2-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->causal-conv1d) (12.6.77)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->causal-conv1d) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->causal-conv1d) (1.3.0)\n",
            "Downloading ninja-1.11.1.2-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: causal-conv1d\n",
            "  Building wheel for causal-conv1d (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for causal-conv1d: filename=causal_conv1d-1.4.0-cp310-cp310-linux_x86_64.whl size=104867883 sha256=b5e7cf7e964b5e99275d97ba1e1b0ee4e3073f4593743ba1f1c6aa394a3008cc\n",
            "  Stored in directory: /root/.cache/pip/wheels/e3/dd/4c/205f24e151736bd22f5980738dd10a19af6f093b6f4dcab006\n",
            "Successfully built causal-conv1d\n",
            "Installing collected packages: ninja, causal-conv1d\n",
            "Successfully installed causal-conv1d-1.4.0 ninja-1.11.1.2\n",
            "Collecting mamba-ssm\n",
            "  Downloading mamba_ssm-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (2.4.0+cu121)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (24.2)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (1.11.1.2)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (0.8.0)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (3.0.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (4.46.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->mamba-ssm) (12.6.77)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (1.26.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (4.66.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->mamba-ssm) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->mamba-ssm) (1.3.0)\n",
            "Building wheels for collected packages: mamba-ssm\n",
            "  Building wheel for mamba-ssm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mamba-ssm: filename=mamba_ssm-2.2.2-cp310-cp310-linux_x86_64.whl size=323988104 sha256=6b082468a6abb6f6bc50c99263f17c6c7f5a2e8f6b275ed7998b81fb25279229\n",
            "  Stored in directory: /root/.cache/pip/wheels/57/7c/90/9f963468ecc3791e36e388f9e7b4a4e1e3f90fbb340055aa4d\n",
            "Successfully built mamba-ssm\n",
            "Installing collected packages: mamba-ssm\n",
            "Successfully installed mamba-ssm-2.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from mamba_ssm import Mamba  # Assuming Mamba is installed\n",
        "import math\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozvbuj_15knF",
        "outputId": "7acb5be3-0499-4d6a-b76f-8f0487516b96"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/selective_scan_interface.py:164: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  def forward(ctx, xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n",
            "/usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/selective_scan_interface.py:240: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  def backward(ctx, dout):\n",
            "/usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/triton/layer_norm.py:986: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  def forward(\n",
            "/usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/triton/layer_norm.py:1045: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  def backward(ctx, dout, *args):\n",
            "/usr/local/lib/python3.10/dist-packages/mamba_ssm/distributed/tensor_parallel.py:26: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  def forward(ctx, x, weight, bias, process_group=None, sequence_parallel=True):\n",
            "/usr/local/lib/python3.10/dist-packages/mamba_ssm/distributed/tensor_parallel.py:62: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  def backward(ctx, grad_output):\n",
            "/usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:758: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  def forward(ctx, zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states=None, seq_idx=None, dt_limit=(0.0, float(\"inf\")), return_final_states=False, activation=\"silu\",\n",
            "/usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:836: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  def backward(ctx, dout, *args):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.pe = pe.unsqueeze(0).to(device)  # Shape: (1, max_len, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :]  # Add positional encoding\n",
        "\n",
        "\n",
        "class MambaAttentionClassifier(nn.Module):\n",
        "    def __init__(self, ts_feature_dim, static_feature_dim, hidden_dim, num_classes):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            ts_feature_dim (int): Number of features in time-series data (e.g., 37).\n",
        "            static_feature_dim (int): Number of features in static data (e.g., 8).\n",
        "            hidden_dim (int): Dimension of hidden states in the model.\n",
        "            num_classes (int): Number of output classes (e.g., 2 for binary classification).\n",
        "        \"\"\"\n",
        "        super(MambaAttentionClassifier, self).__init__()\n",
        "\n",
        "        # Time-series processing with Mamba\n",
        "        self.positional_encoding = PositionalEncoding(d_model=ts_feature_dim)\n",
        "        self.mamba_layer = Mamba(\n",
        "            d_model=ts_feature_dim,  # Include time as an additional feature\n",
        "            d_state=hidden_dim,         # Mamba's internal state size\n",
        "            d_conv=4,                   # Convolution width for local dependencies\n",
        "            expand=2                    # Expansion factor\n",
        "        )\n",
        "\n",
        "        self.projection = nn.Linear(ts_feature_dim, hidden_dim)\n",
        "        self.mamba_norm = nn.LayerNorm(hidden_dim)  # Layer normalization for stability\n",
        "\n",
        "\n",
        "        # Multi-head attention\n",
        "        self.multihead_attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=4, batch_first=True)\n",
        "\n",
        "        # Static feature processing\n",
        "        self.static_fc = nn.Linear(static_feature_dim, hidden_dim)\n",
        "        self.static_norm = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "        # Fully connected layers for classification\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim // 2, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, ts_values, ts_indicators, ts_time, static):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            ts_values (torch.Tensor): Time-series data (batch_size, seq_len, ts_feature_dim).\n",
        "            ts_indicators (torch.Tensor): Indicator for missing time-series data (batch_size, seq_len, ts_feature_dim).\n",
        "            ts_time (torch.Tensor): Time-series timestamps (batch_size, seq_len).\n",
        "            static (torch.Tensor): Static features (batch_size, static_feature_dim).\n",
        "        Returns:\n",
        "            torch.Tensor: Class probabilities (batch_size, num_classes).\n",
        "        \"\"\"\n",
        "        # Ensure the shape of ts_indicators matches ts_values\n",
        "        assert ts_values.shape == ts_indicators.shape, \"Shape mismatch between ts_values and ts_indicators\"\n",
        "\n",
        "        # Handle missing data: Mask out the missing time-series values using ts_indicators\n",
        "        ts_values = ts_values * ts_indicators  # Element-wise multiplication to mask missing data\n",
        "\n",
        "        # Add time as an additional feature and apply positional encoding\n",
        "        ts_time = ts_time.unsqueeze(-1)  # (batch_size, seq_len, 1)\n",
        "        ts_combined = torch.cat([ts_values, ts_time], dim=-1)  # (batch_size, seq_len, ts_feature_dim + 1)\n",
        "        ts_combined = self.positional_encoding(ts_combined)\n",
        "\n",
        "        # Process time-series data with Mamba\n",
        "        ts_encoded = self.mamba_layer(ts_combined)  # (batch_size, seq_len, hidden_dim)\n",
        "        ts_encoded = self.projection(ts_encoded)  # (batch_size, seq_len, hidden_dim)\n",
        "        ts_encoded = self.mamba_norm(ts_encoded)  # Normalize the Mamba output\n",
        "\n",
        "\n",
        "        # Apply multi-head attention\n",
        "        ts_encoded, _ = self.multihead_attention(ts_encoded, ts_encoded, ts_encoded)\n",
        "\n",
        "        # Compute attention weights\n",
        "        attn_weights = F.softmax(torch.mean(ts_encoded, dim=-1, keepdim=True), dim=1)  # (batch_size, seq_len, 1)\n",
        "        ts_attended = torch.sum(attn_weights * ts_encoded, dim=1)  # (batch_size, hidden_dim)\n",
        "\n",
        "        # Process static features\n",
        "        static_encoded = F.relu(self.static_fc(static))  # (batch_size, hidden_dim)\n",
        "        static_encoded = self.static_norm(static_encoded)\n",
        "\n",
        "        # Concatenate attended time-series and static features\n",
        "        combined = torch.cat([ts_attended, static_encoded], dim=1)  # (batch_size, hidden_dim * 2)\n",
        "\n",
        "        # Classification\n",
        "        output = self.classifier(combined)  # (batch_size, num_classes)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "O3Tm30CZylSa"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MambaAttentionClassifier(nn.Module):\n",
        "    def __init__(self, ts_feature_dim, static_feature_dim, hidden_dim, num_classes):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            ts_feature_dim (int): Number of features in time-series data (e.g., 37).\n",
        "            static_feature_dim (int): Number of features in static data (e.g., 8).\n",
        "            hidden_dim (int): Dimension of hidden states in the model.\n",
        "            num_classes (int): Number of output classes (e.g., 2 for binary classification).\n",
        "        \"\"\"\n",
        "        super(MambaAttentionClassifier, self).__init__()\n",
        "\n",
        "        # Time-series processing with Mamba\n",
        "        self.mamba_layer = Mamba(\n",
        "            d_model=ts_feature_dim,  # Include time as an additional feature\n",
        "            d_state=hidden_dim,         # Mamba's internal state size\n",
        "            d_conv=4,                   # Convolution width for local dependencies\n",
        "            expand=2                    # Expansion factor\n",
        "        )\n",
        "\n",
        "        # Static feature processing\n",
        "        self.static_fc = nn.Linear(static_feature_dim, hidden_dim)\n",
        "\n",
        "        # Attention layer to weight time-series features\n",
        "        self.attention = nn.Linear(ts_feature_dim, 1)\n",
        "\n",
        "        # Fully connected layers for classification\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(ts_feature_dim + hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, ts_values, ts_indicators, ts_time, static):\n",
        "      \"\"\"\n",
        "      Args:\n",
        "          ts_values (torch.Tensor): Time-series data (batch_size, seq_len, ts_feature_dim).\n",
        "          ts_indicators (torch.Tensor): Indicator for missing time-series data (batch_size, seq_len, ts_feature_dim).\n",
        "          ts_time (torch.Tensor): Time-series timestamps (batch_size, seq_len).\n",
        "          static (torch.Tensor): Static features (batch_size, static_feature_dim).\n",
        "      Returns:\n",
        "          torch.Tensor: Class probabilities (batch_size, num_classes).\n",
        "      \"\"\"\n",
        "      # Ensure the shape of ts_indicators matches ts_values\n",
        "      assert ts_values.shape == ts_indicators.shape, \"Shape mismatch between ts_values and ts_indicators\"\n",
        "\n",
        "      # Handle missing data: Mask out the missing time-series values using ts_indicators\n",
        "      ts_values = ts_values * ts_indicators  # Element-wise multiplication to mask missing data\n",
        "\n",
        "      ts_time = ts_time.unsqueeze(-1)  # (batch_size, seq_len, 1)\n",
        "      ts_combined = torch.cat([ts_values, ts_time], dim=-1)  # (batch_size, seq_len, ts_feature_dim + 1)\n",
        "\n",
        "      # Process time-series data with Mamba\n",
        "      ts_encoded = self.mamba_layer(ts_combined)  # (batch_size, seq_len, hidden_dim)\n",
        "\n",
        "      # print('ts_encoded shape',ts_encoded.shape)\n",
        "\n",
        "      # Reshape ts_encoded for the attention layer\n",
        "      batch_size, seq_len, hidden_dim = ts_encoded.shape\n",
        "      ts_encoded_flat = ts_encoded.view(-1, hidden_dim)  # Flatten to (batch_size * seq_len, hidden_dim)\n",
        "\n",
        "      # Compute attention scores\n",
        "      attn_scores = self.attention(ts_encoded_flat)  # (batch_size * seq_len, 1)\n",
        "      attn_scores = attn_scores.view(batch_size, seq_len, 1)  # Reshape back to (batch_size, seq_len, 1)\n",
        "\n",
        "      # Compute attention weights\n",
        "      attn_weights = F.softmax(attn_scores, dim=1)  # (batch_size, seq_len, 1)\n",
        "\n",
        "      # Apply attention weights to the Mamba output\n",
        "      ts_attended = torch.sum(attn_weights * ts_encoded, dim=1)  # (batch_size, hidden_dim)\n",
        "\n",
        "      # Process static features\n",
        "      static_encoded = F.relu(self.static_fc(static))  # (batch_size, hidden_dim)\n",
        "\n",
        "      # Concatenate attended time-series and static features\n",
        "      combined = torch.cat([ts_attended, static_encoded], dim=1)  # (batch_size, hidden_dim * 2)\n",
        "\n",
        "      # print('ts_attended shape',ts_attended.shape)\n",
        "      # print('static_encoded shape', static_encoded.shape)\n",
        "      # print('combined shape', combined.shape)\n",
        "\n",
        "      # Classification\n",
        "      output = self.classifier(combined)  # (batch_size, num_classes)\n",
        "\n",
        "      return output\n",
        "\n"
      ],
      "metadata": {
        "id": "UIG39zTTtHxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drive.mount('/content/drive')\n",
        "# project_dir = '/content/drive/My Drive/ssm_ehr'\n",
        "train_data = np.load('/content/drive/MyDrive/ssm_ehr/datasets/split_1/train_physionet2012_1.npy', allow_pickle=True)\n",
        "test_data = np.load('/content/drive/MyDrive/ssm_ehr/datasets/split_1/train_physionet2012_1.npy', allow_pickle=True)\n",
        "val_data = np.load('/content/drive/MyDrive/ssm_ehr/datasets/split_1/train_physionet2012_1.npy', allow_pickle=True)"
      ],
      "metadata": {
        "id": "rIwaoK-Gy0fx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def custom_collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Custom collate function to handle batches with variable-length time-series data and static features.\n",
        "\n",
        "    Args:\n",
        "        batch (list of tuples): Each tuple contains (ts_values, ts_indicators, ts_time, static, labels).\n",
        "\n",
        "    Returns:\n",
        "        tuple: Padded time-series values, indicators, times, static features, and labels.\n",
        "    \"\"\"\n",
        "    ts_values = [sample[0].clone().detach().float() for sample in batch]\n",
        "    ts_indicators = [sample[1].clone().detach().float() for sample in batch]\n",
        "    ts_times = [sample[2].clone().detach().float() for sample in batch]\n",
        "    static = torch.stack([sample[3].clone().detach().float() for sample in batch])\n",
        "    labels = torch.tensor([sample[4] for sample in batch], dtype=torch.float32)\n",
        "\n",
        "    # Pad ts_values, ts_indicators, and ts_time\n",
        "    ts_values_padded = pad_sequence(ts_values, batch_first=True)\n",
        "    ts_indicators_padded = pad_sequence(ts_indicators, batch_first=True)\n",
        "    ts_times_padded = pad_sequence(ts_times, batch_first=True)\n",
        "\n",
        "    return ts_values_padded, ts_indicators_padded, ts_times_padded, static, labels"
      ],
      "metadata": {
        "id": "wsjLVeS-xTkG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ICUTimeSeriesDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        return (\n",
        "            torch.tensor(sample['ts_values'], dtype=torch.float32),  # Time-series values\n",
        "            torch.tensor(sample['ts_indicators'], dtype=torch.float32),  # Missing indicators\n",
        "            torch.tensor(sample['ts_times'], dtype=torch.float32),  # Time steps\n",
        "            torch.tensor(sample['static'], dtype=torch.float32),  # Static features\n",
        "            torch.tensor(sample['labels'], dtype=torch.float32)  # Label\n",
        "        )\n",
        "\n",
        "# Create train, validation, and test datasets\n",
        "# train_data = np.load('Data/P12Data_1/split_1/train_physionet2012_1.npy', allow_pickle=True)\n",
        "# test_data = np.load('Data/P12Data_1/split_1/test_physionet2012_1.npy', allow_pickle=True)\n",
        "# val_data = np.load('Data/P12Data_1/split_1/validation_physionet2012_1.npy', allow_pickle=True)\n",
        "\n",
        "train_dataset = ICUTimeSeriesDataset(train_data)\n",
        "val_dataset = ICUTimeSeriesDataset(val_data)\n",
        "test_dataset = ICUTimeSeriesDataset(test_data)\n",
        "\n",
        "# Dataloader\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=custom_collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=custom_collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=custom_collate_fn)"
      ],
      "metadata": {
        "id": "3JIm0_MdxXIu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIOckuB_0ZBp",
        "outputId": "c10893b3-0a56-43c4-b20c-35a22533dfe5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Define model\n",
        "model = MambaAttentionClassifier(\n",
        "    ts_feature_dim=38,\n",
        "    static_feature_dim=8,\n",
        "    hidden_dim=64,\n",
        "    num_classes=2\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(100):  # Adjust epochs as needed\n",
        "    model.train()\n",
        "    for ts_values, ts_indicators, ts_time, static, labels in train_loader:\n",
        "        ts_values,ts_indicators, ts_time , static, labels = ts_values.to(device), ts_indicators.to(device), ts_time.to(device), static.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(ts_values, ts_indicators, ts_time, static)\n",
        "        loss = criterion(outputs, labels.long())\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/100], Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RW--6rwxYj_",
        "outputId": "e3e2a46a-175a-40f7-9a37-a58d6b4bd15c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: 0.6214\n",
            "Epoch [2/100], Loss: 0.2667\n",
            "Epoch [3/100], Loss: 0.3618\n",
            "Epoch [4/100], Loss: 0.2018\n",
            "Epoch [5/100], Loss: 0.2690\n",
            "Epoch [6/100], Loss: 0.3858\n",
            "Epoch [7/100], Loss: 0.2078\n",
            "Epoch [8/100], Loss: 0.1243\n",
            "Epoch [9/100], Loss: 0.6300\n",
            "Epoch [10/100], Loss: 0.1670\n",
            "Epoch [11/100], Loss: 0.3674\n",
            "Epoch [12/100], Loss: 0.1851\n",
            "Epoch [13/100], Loss: 0.2427\n",
            "Epoch [14/100], Loss: 0.2393\n",
            "Epoch [15/100], Loss: 0.2748\n",
            "Epoch [16/100], Loss: 0.5315\n",
            "Epoch [17/100], Loss: 0.2089\n",
            "Epoch [18/100], Loss: 0.1379\n",
            "Epoch [19/100], Loss: 0.1769\n",
            "Epoch [20/100], Loss: 0.2095\n",
            "Epoch [21/100], Loss: 0.2463\n",
            "Epoch [22/100], Loss: 0.0853\n",
            "Epoch [23/100], Loss: 0.3786\n",
            "Epoch [24/100], Loss: 0.2540\n",
            "Epoch [25/100], Loss: 0.0718\n",
            "Epoch [26/100], Loss: 0.2231\n",
            "Epoch [27/100], Loss: 0.3472\n",
            "Epoch [28/100], Loss: 0.0942\n",
            "Epoch [29/100], Loss: 0.1795\n",
            "Epoch [30/100], Loss: 0.5082\n",
            "Epoch [31/100], Loss: 0.1834\n",
            "Epoch [32/100], Loss: 0.1393\n",
            "Epoch [33/100], Loss: 0.0387\n",
            "Epoch [34/100], Loss: 0.1058\n",
            "Epoch [35/100], Loss: 0.2608\n",
            "Epoch [36/100], Loss: 0.0771\n",
            "Epoch [37/100], Loss: 0.5742\n",
            "Epoch [38/100], Loss: 0.1705\n",
            "Epoch [39/100], Loss: 0.1218\n",
            "Epoch [40/100], Loss: 0.1232\n",
            "Epoch [41/100], Loss: 0.2546\n",
            "Epoch [42/100], Loss: 0.0787\n",
            "Epoch [43/100], Loss: 0.3027\n",
            "Epoch [44/100], Loss: 0.0949\n",
            "Epoch [45/100], Loss: 0.3399\n",
            "Epoch [46/100], Loss: 0.1182\n",
            "Epoch [47/100], Loss: 0.0959\n",
            "Epoch [48/100], Loss: 0.0409\n",
            "Epoch [49/100], Loss: 0.1470\n",
            "Epoch [50/100], Loss: 0.0444\n",
            "Epoch [51/100], Loss: 0.1108\n",
            "Epoch [52/100], Loss: 0.0163\n",
            "Epoch [53/100], Loss: 0.1159\n",
            "Epoch [54/100], Loss: 0.2779\n",
            "Epoch [55/100], Loss: 0.0132\n",
            "Epoch [56/100], Loss: 0.0698\n",
            "Epoch [57/100], Loss: 0.1735\n",
            "Epoch [58/100], Loss: 0.0626\n",
            "Epoch [59/100], Loss: 0.2101\n",
            "Epoch [60/100], Loss: 0.1683\n",
            "Epoch [61/100], Loss: 0.0208\n",
            "Epoch [62/100], Loss: 0.0983\n",
            "Epoch [63/100], Loss: 0.0168\n",
            "Epoch [64/100], Loss: 0.0718\n",
            "Epoch [65/100], Loss: 0.0339\n",
            "Epoch [66/100], Loss: 0.1718\n",
            "Epoch [67/100], Loss: 0.0560\n",
            "Epoch [68/100], Loss: 0.0166\n",
            "Epoch [69/100], Loss: 0.0297\n",
            "Epoch [70/100], Loss: 0.0457\n",
            "Epoch [71/100], Loss: 0.0158\n",
            "Epoch [72/100], Loss: 0.0455\n",
            "Epoch [73/100], Loss: 0.0452\n",
            "Epoch [74/100], Loss: 0.0521\n",
            "Epoch [75/100], Loss: 0.3199\n",
            "Epoch [76/100], Loss: 0.0247\n",
            "Epoch [77/100], Loss: 0.0199\n",
            "Epoch [78/100], Loss: 0.0916\n",
            "Epoch [79/100], Loss: 0.2954\n",
            "Epoch [80/100], Loss: 0.0149\n",
            "Epoch [81/100], Loss: 0.0118\n",
            "Epoch [82/100], Loss: 0.0359\n",
            "Epoch [83/100], Loss: 0.2889\n",
            "Epoch [84/100], Loss: 0.0626\n",
            "Epoch [85/100], Loss: 0.0965\n",
            "Epoch [86/100], Loss: 0.0127\n",
            "Epoch [87/100], Loss: 0.0028\n",
            "Epoch [88/100], Loss: 0.0038\n",
            "Epoch [89/100], Loss: 0.0375\n",
            "Epoch [90/100], Loss: 0.0296\n",
            "Epoch [91/100], Loss: 0.0240\n",
            "Epoch [92/100], Loss: 0.0068\n",
            "Epoch [93/100], Loss: 0.0049\n",
            "Epoch [94/100], Loss: 0.0186\n",
            "Epoch [95/100], Loss: 0.0099\n",
            "Epoch [96/100], Loss: 0.0191\n",
            "Epoch [97/100], Loss: 0.0031\n",
            "Epoch [98/100], Loss: 0.0044\n",
            "Epoch [99/100], Loss: 0.1003\n",
            "Epoch [100/100], Loss: 0.0090\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation loop\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for ts_values, ts_indicators, ts_time, static, labels in val_loader:\n",
        "        ts_values,ts_indicators,ts_time, static, labels = ts_values.to(device), ts_indicators.to(device),ts_time.to(device), static.to(device), labels.to(device)\n",
        "        outputs = model(ts_values, ts_indicators,ts_time, static)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels.long()).sum().item()\n",
        "\n",
        "    print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6WTtyoNxZ2G",
        "outputId": "61a66486-3767-4f95-aa17-73a1ee030418"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 99.16%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, data_loader, device):\n",
        "    \"\"\"\n",
        "    Evaluates the model on the given data loader and calculates evaluation metrics.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): Trained model.\n",
        "        data_loader (torch.utils.data.DataLoader): Data loader for validation/test set.\n",
        "        device (torch.device): Device to perform computation on (CPU/GPU).\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing evaluation metrics.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    y_prob = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for ts_values, ts_indicators, ts_time, static, labels in data_loader:\n",
        "            # Move data to device\n",
        "            ts_values,ts_indicators, ts_time, static, labels = ts_values.to(device), ts_indicators.to(device), ts_time.to(device), static.to(device), labels.to(device)\n",
        "\n",
        "            # Get model predictions\n",
        "            outputs = model(ts_values, ts_indicators, ts_time, static)  # Raw logits\n",
        "            probabilities = torch.softmax(outputs, dim=1)[:, 1]  # Probability for class 1\n",
        "            predictions = torch.argmax(outputs, dim=1)  # Predicted class labels\n",
        "\n",
        "            # Collect predictions and ground truth\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(predictions.cpu().numpy())\n",
        "            y_prob.extend(probabilities.cpu().numpy())\n",
        "\n",
        "    # Calculate evaluation metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, zero_division=1)\n",
        "    recall = recall_score(y_true, y_pred, zero_division=1)\n",
        "    f1 = f1_score(y_true, y_pred, zero_division=1)\n",
        "    roc_auc = roc_auc_score(y_true, y_prob)\n",
        "\n",
        "    return {\n",
        "        \"Accuracy\": accuracy,\n",
        "        \"Precision\": precision,\n",
        "        \"Recall\": recall,\n",
        "        \"F1-Score\": f1,\n",
        "        \"ROC-AUC\": roc_auc,\n",
        "    }"
      ],
      "metadata": {
        "id": "21P4H7sdxbFH"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on the validation or test set\n",
        "metrics = evaluate_model(model, val_loader, device)\n",
        "\n",
        "# Print metrics\n",
        "for metric, value in metrics.items():\n",
        "    print(f\"{metric}: {value:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRKLRIpQxcWP",
        "outputId": "4193bbf9-1cda-4342-fda6-a69f626a0039"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9916\n",
            "Precision: 0.9612\n",
            "Recall: 0.9791\n",
            "F1-Score: 0.9701\n",
            "ROC-AUC: 0.9992\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u8Z_uymw9s0x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}