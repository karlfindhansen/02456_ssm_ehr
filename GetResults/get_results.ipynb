{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21827,
     "status": "ok",
     "timestamp": 1733347453311,
     "user": {
      "displayName": "David Miles-Skov",
      "userId": "17994023048384114212"
     },
     "user_tz": -60
    },
    "id": "VigXREV0sdsK",
    "outputId": "ee01fed8-46cd-49a0-c0da-75f4841c5ad4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "drive.mount('/content/drive')\n",
    "project_dir = '/content/drive/My Drive/ssm_ehr'\n",
    "print(os.path.exists(project_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1733347453311,
     "user": {
      "displayName": "David Miles-Skov",
      "userId": "17994023048384114212"
     },
     "user_tz": -60
    },
    "id": "hO-kAOqotHYo",
    "outputId": "7ac88706-9011-4f2e-d8f5-493eb4110d9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 181064,
     "status": "ok",
     "timestamp": 1733347634372,
     "user": {
      "displayName": "David Miles-Skov",
      "userId": "17994023048384114212"
     },
     "user_tz": -60
    },
    "id": "OT70cV1KxKtC",
    "outputId": "d8ebca67-790c-4201-c6dc-8841e7558d42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Collecting torch==2.4.0\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torch-2.4.0%2Bcu121-cp310-cp310-linux_x86_64.whl (799.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m799.1/799.1 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torchvision==0.19.0\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.19.0%2Bcu121-cp310-cp310-linux_x86_64.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m111.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torchaudio==2.4.0\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.4.0%2Bcu121-cp310-cp310-linux_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m102.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (2024.10.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m93.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m112.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.0)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m285.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting triton==3.0.0 (from torch==2.4.0)\n",
      "  Downloading https://download.pytorch.org/whl/triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.19.0) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.19.0) (11.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0) (12.6.77)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.0) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.0) (1.3.0)\n",
      "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, torchaudio\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
      "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
      "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.6.3.3\n",
      "    Uninstalling nvidia-cublas-cu12-12.6.3.3:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.6.3.3\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.5.1.17\n",
      "    Uninstalling nvidia-cudnn-cu12-9.5.1.17:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.5.1.17\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.5.1+cu121\n",
      "    Uninstalling torch-2.5.1+cu121:\n",
      "      Successfully uninstalled torch-2.5.1+cu121\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.20.1+cu121\n",
      "    Uninstalling torchvision-0.20.1+cu121:\n",
      "      Successfully uninstalled torchvision-0.20.1+cu121\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.5.1+cu121\n",
      "    Uninstalling torchaudio-2.5.1+cu121:\n",
      "      Successfully uninstalled torchaudio-2.5.1+cu121\n",
      "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.4.0+cu121 torchaudio-2.4.0+cu121 torchvision-0.19.0+cu121 triton-3.0.0\n",
      "\u001b[33mWARNING: Skipping mamba-ssm as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping causal-conv1d as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting causal-conv1d\n",
      "  Downloading causal_conv1d-1.4.0.tar.gz (9.3 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from causal-conv1d) (2.4.0+cu121)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from causal-conv1d) (24.2)\n",
      "Collecting ninja (from causal-conv1d)\n",
      "  Downloading ninja-1.11.1.2-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->causal-conv1d) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->causal-conv1d) (12.6.77)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->causal-conv1d) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->causal-conv1d) (1.3.0)\n",
      "Downloading ninja-1.11.1.2-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: causal-conv1d\n",
      "  Building wheel for causal-conv1d (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for causal-conv1d: filename=causal_conv1d-1.4.0-cp310-cp310-linux_x86_64.whl size=104867883 sha256=b5e7cf7e964b5e99275d97ba1e1b0ee4e3073f4593743ba1f1c6aa394a3008cc\n",
      "  Stored in directory: /root/.cache/pip/wheels/e3/dd/4c/205f24e151736bd22f5980738dd10a19af6f093b6f4dcab006\n",
      "Successfully built causal-conv1d\n",
      "Installing collected packages: ninja, causal-conv1d\n",
      "Successfully installed causal-conv1d-1.4.0 ninja-1.11.1.2\n",
      "Collecting mamba-ssm\n",
      "  Downloading mamba_ssm-2.2.2.tar.gz (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (2.4.0+cu121)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (24.2)\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (1.11.1.2)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (0.8.0)\n",
      "Requirement already satisfied: triton in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (3.0.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from mamba-ssm) (4.46.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->mamba-ssm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->mamba-ssm) (12.6.77)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (1.26.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba-ssm) (4.66.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->mamba-ssm) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba-ssm) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->mamba-ssm) (1.3.0)\n",
      "Building wheels for collected packages: mamba-ssm\n",
      "  Building wheel for mamba-ssm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for mamba-ssm: filename=mamba_ssm-2.2.2-cp310-cp310-linux_x86_64.whl size=323988104 sha256=6b082468a6abb6f6bc50c99263f17c6c7f5a2e8f6b275ed7998b81fb25279229\n",
      "  Stored in directory: /root/.cache/pip/wheels/57/7c/90/9f963468ecc3791e36e388f9e7b4a4e1e3f90fbb340055aa4d\n",
      "Successfully built mamba-ssm\n",
      "Installing collected packages: mamba-ssm\n",
      "Successfully installed mamba-ssm-2.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip uninstall mamba-ssm causal-conv1d\n",
    "!pip install causal-conv1d && pip install mamba-ssm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5218,
     "status": "ok",
     "timestamp": 1733347639584,
     "user": {
      "displayName": "David Miles-Skov",
      "userId": "17994023048384114212"
     },
     "user_tz": -60
    },
    "id": "ozvbuj_15knF",
    "outputId": "7a73862c-5ade-4a14-dd56-f02192a6a2ea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/selective_scan_interface.py:164: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n",
      "/usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/selective_scan_interface.py:240: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, dout):\n",
      "/usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/triton/layer_norm.py:986: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(\n",
      "/usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/triton/layer_norm.py:1045: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, dout, *args):\n",
      "/usr/local/lib/python3.10/dist-packages/mamba_ssm/distributed/tensor_parallel.py:26: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, x, weight, bias, process_group=None, sequence_parallel=True):\n",
      "/usr/local/lib/python3.10/dist-packages/mamba_ssm/distributed/tensor_parallel.py:62: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:758: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states=None, seq_idx=None, dt_limit=(0.0, float(\"inf\")), return_final_states=False, activation=\"silu\",\n",
      "/usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:836: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, dout, *args):\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from mamba_ssm import Mamba  # Assuming Mamba is installed\n",
    "import math\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TWn5NXaA4PQ-"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j3VThDmaueHt"
   },
   "outputs": [],
   "source": [
    "class MoEMambaAttentionClassifier(nn.Module):\n",
    "    def __init__(self, ts_feature_dim, static_feature_dim, hidden_dim, num_classes, num_experts=4, max_time_steps=1000):\n",
    "        super(MoEMambaAttentionClassifier, self).__init__()\n",
    "\n",
    "        self.num_experts = num_experts\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Gating network\n",
    "        self.gating_network = nn.Sequential(\n",
    "            nn.Linear(ts_feature_dim, num_experts),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "        # Input projection to match hidden_dim\n",
    "        self.input_projection = nn.Linear(ts_feature_dim, hidden_dim)\n",
    "\n",
    "        # Define experts (using Mamba layers here)\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                Mamba(d_model=hidden_dim, d_state=hidden_dim, d_conv=4, expand=2),\n",
    "                nn.LayerNorm(hidden_dim),\n",
    "                nn.Dropout(0.3)\n",
    "            )\n",
    "            for _ in range(num_experts)\n",
    "        ])\n",
    "\n",
    "        # Remaining layers (similar to your current model)\n",
    "        self.projection = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.multihead_attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=4, dropout=0.2, batch_first=True)\n",
    "        self.attention_layer = nn.Linear(hidden_dim, 1)\n",
    "        self.static_fc = nn.utils.weight_norm(nn.Linear(static_feature_dim, hidden_dim))\n",
    "        self.static_norm = nn.LayerNorm(hidden_dim)\n",
    "        # self.classifier = nn.Sequential(\n",
    "        #     nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Dropout(0.5),\n",
    "        #     nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(hidden_dim // 2, num_classes)\n",
    "        # )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1),  # Single output for binary classification\n",
    "            nn.Sigmoid()                   # Sigmoid activation for probabilities\n",
    "        )\n",
    "\n",
    "    def forward(self, ts_values, ts_indicators, ts_time, static):\n",
    "      # Mask missing time-series values\n",
    "      ts_values = ts_values * ts_indicators\n",
    "\n",
    "      # Compute gating scores using the original ts_values (before projection)\n",
    "      gating_scores = self.gating_network(ts_values.mean(dim=1))  # Shape: (batch_size, num_experts)\n",
    "      gating_weights = F.softmax(gating_scores, dim=-1)  # Shape: (batch_size, num_experts)\n",
    "\n",
    "      # Project input to match hidden_dim for experts\n",
    "      ts_values_projected = self.input_projection(ts_values)  # Shape: (batch_size, seq_len, hidden_dim)\n",
    "\n",
    "      # Expert outputs\n",
    "      expert_outputs = []\n",
    "      for i, expert in enumerate(self.experts):\n",
    "          expert_output = expert(ts_values_projected)  # Shape: (batch_size, seq_len, hidden_dim)\n",
    "          expert_outputs.append(expert_output)\n",
    "      expert_outputs = torch.stack(expert_outputs, dim=1)  # Shape: (batch_size, num_experts, seq_len, hidden_dim)\n",
    "\n",
    "      # Combine expert outputs using gating weights\n",
    "      ts_encoded = torch.einsum('be,besh->bsh', gating_weights, expert_outputs)  # Weighted sum\n",
    "\n",
    "      # Multi-head attention\n",
    "      attn_output, _ = self.multihead_attention(ts_encoded, ts_encoded, ts_encoded)\n",
    "      ts_encoded = ts_encoded + attn_output  # Residual connection\n",
    "\n",
    "      # Compute learnable attention weights\n",
    "      attn_scores = self.attention_layer(ts_encoded).squeeze(-1)\n",
    "      attn_weights = F.softmax(attn_scores, dim=1)\n",
    "      ts_attended = torch.sum(ts_encoded * attn_weights.unsqueeze(-1), dim=1)\n",
    "\n",
    "      # Static features\n",
    "      static_encoded = F.relu(self.static_fc(static))\n",
    "      static_encoded = self.static_norm(static_encoded)\n",
    "\n",
    "      # Concatenate features\n",
    "      combined = torch.cat([ts_attended, static_encoded], dim=1)\n",
    "\n",
    "      # Classification\n",
    "      output = self.classifier(combined)\n",
    "      return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rIwaoK-Gy0fx"
   },
   "outputs": [],
   "source": [
    "# drive.mount('/content/drive')\n",
    "# project_dir = '/content/drive/My Drive/ssm_ehr'\n",
    "train_data = np.load('/content/drive/MyDrive/ssm_ehr/datasets/split_1/train_physionet2012_1.npy', allow_pickle=True)\n",
    "test_data = np.load('/content/drive/MyDrive/ssm_ehr/datasets/split_1/test_physionet2012_1.npy', allow_pickle=True)\n",
    "val_data = np.load('/content/drive/MyDrive/ssm_ehr/datasets/split_1/validation_physionet2012_1.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wsjLVeS-xTkG"
   },
   "outputs": [],
   "source": [
    "\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle batches with variable-length time-series data and static features.\n",
    "\n",
    "    Args:\n",
    "        batch (list of tuples): Each tuple contains (ts_values, ts_indicators, ts_time, static, labels).\n",
    "\n",
    "    Returns:\n",
    "        tuple: Padded time-series values, indicators, times, static features, and labels.\n",
    "    \"\"\"\n",
    "    ts_values = [sample[0].clone().detach().float() for sample in batch]\n",
    "    ts_indicators = [sample[1].clone().detach().float() for sample in batch]\n",
    "    ts_times = [sample[2].clone().detach().float() for sample in batch]\n",
    "    static = torch.stack([sample[3].clone().detach().float() for sample in batch])\n",
    "    labels = torch.tensor([sample[4] for sample in batch], dtype=torch.float32)\n",
    "\n",
    "    # Pad ts_values, ts_indicators, and ts_time\n",
    "    ts_values_padded = pad_sequence(ts_values, batch_first=True)\n",
    "    ts_indicators_padded = pad_sequence(ts_indicators, batch_first=True)\n",
    "    ts_times_padded = pad_sequence(ts_times, batch_first=True)\n",
    "\n",
    "    return ts_values_padded, ts_indicators_padded, ts_times_padded, static, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3JIm0_MdxXIu"
   },
   "outputs": [],
   "source": [
    "\n",
    "class ICUTimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        return (\n",
    "            torch.tensor(sample['ts_values'], dtype=torch.float32),  # Time-series values\n",
    "            torch.tensor(sample['ts_indicators'], dtype=torch.float32),  # Missing indicators\n",
    "            torch.tensor(sample['ts_times'], dtype=torch.float32),  # Time steps\n",
    "            torch.tensor(sample['static'], dtype=torch.float32),  # Static features\n",
    "            torch.tensor(sample['labels'], dtype=torch.float32)  # Label\n",
    "        )\n",
    "\n",
    "\n",
    "train_dataset = ICUTimeSeriesDataset(train_data)\n",
    "val_dataset = ICUTimeSeriesDataset(val_data)\n",
    "test_dataset = ICUTimeSeriesDataset(test_data)\n",
    "\n",
    "# Dataloader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1733347647642,
     "user": {
      "displayName": "David Miles-Skov",
      "userId": "17994023048384114212"
     },
     "user_tz": -60
    },
    "id": "uIOckuB_0ZBp",
    "outputId": "02d3f2fa-775a-4d4a-c046-45a674cd40d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "executionInfo": {
     "elapsed": 436,
     "status": "error",
     "timestamp": 1733347946722,
     "user": {
      "displayName": "David Miles-Skov",
      "userId": "17994023048384114212"
     },
     "user_tz": -60
    },
    "id": "3RW--6rwxYj_",
    "outputId": "d5bd98e9-1024-4c24-ca57-97bf0a88123b"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\"host_softmax\" not implemented for 'Long'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-44b72efecc06>\u001b[0m in \u001b[0;36m<cell line: 162>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtest_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maupr_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0mtest_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maupr_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-44b72efecc06>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, num_epochs)\u001b[0m\n\u001b[1;32m     37\u001b[0m           \u001b[0;31m# Apply threshold to convert probabilities to binary predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m           \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m           \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m           \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1189\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m                                label_smoothing=self.label_smoothing)\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3102\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3103\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3104\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \"host_softmax\" not implemented for 'Long'"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Define model\n",
    "\n",
    "model = MoEMambaAttentionClassifier(\n",
    "    ts_feature_dim=37,\n",
    "    static_feature_dim=8,\n",
    "    hidden_dim=16,\n",
    "    num_classes=2\n",
    ")\n",
    "\n",
    "def train(model, train_loader, val_loader, num_epochs = 100):\n",
    "  model.to(device)\n",
    "\n",
    "  # Loss and optimizer\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "  # Training loop\n",
    "  train_losses = []\n",
    "  val_losses = []\n",
    "  for epoch in range(num_epochs):  # Adjust epochs as needed\n",
    "      model.train()\n",
    "      loss_train = 0\n",
    "      for ts_values, ts_indicators, ts_time, static, labels in train_loader:\n",
    "          ts_values,ts_indicators, ts_time , static, labels = ts_values.to(device), ts_indicators.to(device), ts_time.to(device), static.to(device), labels.to(device)\n",
    "\n",
    "\n",
    "          # print(ts_values.shape)\n",
    "          # print(ts_indicators.shape)\n",
    "          # print(ts_time.shape)\n",
    "          # print(static.shape)\n",
    "          # print(labels.shape)\n",
    "          # break\n",
    "\n",
    "          # Forward pass\n",
    "          logits = model(ts_values, ts_indicators, ts_time, static)\n",
    "          # Apply threshold to convert probabilities to binary predictions\n",
    "          predictions = (logits >= 0.5).long()\n",
    "          loss = criterion(predictions, labels)\n",
    "\n",
    "          # Backward pass\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Clip gradients\n",
    "          optimizer.step()\n",
    "\n",
    "          loss_train += loss.item()\n",
    "\n",
    "      train_losses.append(loss_train/len(train_loader))\n",
    "\n",
    "      #validation loss\n",
    "      model.eval().to(device)\n",
    "      labels_list = torch.LongTensor([]).to(device)\n",
    "      predictions_list = torch.FloatTensor([]).to(device)\n",
    "      with torch.no_grad():\n",
    "          for ts_values, ts_indicators, ts_time, static, labels in val_loader:\n",
    "              ts_values,ts_indicators, ts_time, static, labels = ts_values.to(device), ts_indicators.to(device), ts_time.to(device), static.to(device), labels.to(device).long()\n",
    "              labels_list = torch.cat((labels_list, labels), dim=0)\n",
    "              predicition = (model(ts_values, ts_indicators, ts_time, static) >= 0.5).long()\n",
    "              predictions_list = torch.cat((predictions_list, predicition), dim=0)\n",
    "\n",
    "          probs = torch.nn.functional.softmax(predictions_list, dim=1)\n",
    "          auc_score = roc_auc_score(labels_list.cpu().numpy(), probs[:, 1].cpu().numpy())\n",
    "          aupr_score = average_precision_score(labels_list.cpu().numpy(), probs[:, 1].cpu().numpy())\n",
    "          accuracy = accuracy_score(labels_list.cpu().numpy(), (probs[:, 1] >= 0.5).cpu().numpy())\n",
    "\n",
    "      val_loss = criterion(predictions_list, labels_list)\n",
    "      val_losses.append(val_loss)\n",
    "      print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss_train/len(train_loader):.4f}, Val Loss: {val_loss:.4f}, AUC: {auc_score:.4f}, AUPR: {aupr_score:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "      # Validation Loop\n",
    "      model.eval().to(device)\n",
    "      labels_list = torch.LongTensor([]).to(device)\n",
    "      probs_list = torch.FloatTensor([]).to(device)\n",
    "\n",
    "      with torch.no_grad():\n",
    "          for ts_values, ts_indicators, ts_time, static, labels in val_loader:\n",
    "              # Move data to device\n",
    "              ts_values, ts_indicators, ts_time, static, labels = (\n",
    "                  ts_values.to(device),\n",
    "                  ts_indicators.to(device),\n",
    "                  ts_time.to(device),\n",
    "                  static.to(device),\n",
    "                  labels.to(device).long(),\n",
    "              )\n",
    "              # Collect ground truth labels\n",
    "              labels_list = torch.cat((labels_list, labels), dim=0)\n",
    "\n",
    "              # Get model predictions (probabilities)\n",
    "              logits = model(ts_values, ts_indicators, ts_time, static)\n",
    "              probabilities = torch.sigmoid(logits)  # Sigmoid for binary classification\n",
    "              probs_list = torch.cat((probs_list, probabilities), dim=0)\n",
    "\n",
    "          # Calculate metrics\n",
    "          auc_score = roc_auc_score(labels_list.cpu().numpy(), probs_list.cpu().numpy())\n",
    "          aupr_score = average_precision_score(labels_list.cpu().numpy(), probs_list.cpu().numpy())\n",
    "          accuracy = accuracy_score(labels_list.cpu().numpy(), (probs_list >= 0.5).cpu().numpy())\n",
    "\n",
    "          # Calculate validation loss\n",
    "          val_loss = criterion(probs_list, labels_list.float())  # Ensure labels are float for BCE\n",
    "          val_losses.append(val_loss.item())\n",
    "\n",
    "      print(\n",
    "          f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "          f\"Train Loss: {loss_train/len(train_loader):.4f}, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, \"\n",
    "          f\"AUC: {auc_score:.4f}, \"\n",
    "          f\"AUPR: {aupr_score:.4f}, \"\n",
    "          f\"Accuracy: {accuracy:.4f}\"\n",
    "      )\n",
    "\n",
    "  return model, train_losses, val_losses\n",
    "\n",
    "def test(model, test_loader):\n",
    "    model.eval().to(device)  # Set model to evaluation mode\n",
    "\n",
    "    # Loss and metrics\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    test_losses = []\n",
    "    labels_list = torch.LongTensor([]).to(device)\n",
    "    predictions_list = torch.FloatTensor([]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss_test = 0\n",
    "        for ts_values, ts_indicators, ts_time, static, labels in test_loader:\n",
    "            # Move data to device\n",
    "            ts_values, ts_indicators, ts_time, static, labels = (\n",
    "                ts_values.to(device),\n",
    "                ts_indicators.to(device),\n",
    "                ts_time.to(device),\n",
    "                static.to(device),\n",
    "                labels.to(device).long(),\n",
    "            )\n",
    "\n",
    "            # Forward pass\n",
    "            predictions = model(ts_values, ts_indicators, ts_time, static)\n",
    "            loss = criterion(predictions, labels)\n",
    "\n",
    "            # Accumulate test loss\n",
    "            loss_test += loss.item()\n",
    "\n",
    "            # Collect labels and predictions for metrics\n",
    "            labels_list = torch.cat((labels_list, labels), dim=0)\n",
    "            predictions_list = torch.cat((predictions_list, predictions), dim=0)\n",
    "\n",
    "        # Compute average test loss\n",
    "        test_losses.append(loss_test / len(test_loader))\n",
    "\n",
    "        # Compute probabilities for metrics\n",
    "        probs = torch.nn.functional.softmax(predictions_list, dim=1)\n",
    "        auc_score = roc_auc_score(labels_list.cpu().numpy(), probs[:, 1].cpu().numpy())\n",
    "        aupr_score = average_precision_score(labels_list.cpu().numpy(), probs[:, 1].cpu().numpy())\n",
    "        predicted_labels = (probs[:, 1] >= 0.5).cpu().numpy().astype(int)\n",
    "        accuracy = accuracy_score(labels_list.cpu().numpy(), predicted_labels)\n",
    "\n",
    "    # Print test results\n",
    "    print(f\"Test Loss: {test_losses[-1]:.4f}, AUC: {auc_score:.4f}, AUPR: {aupr_score:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    return test_losses, auc_score, aupr_score, accuracy\n",
    "\n",
    "model, train_losses, val_losses = train(model, train_loader, val_loader, 50)\n",
    "print()\n",
    "test_losses, auc_score, aupr_score, accuracy = test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CyhIE_D96ACJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_losses(train_losses, validation_losses):\n",
    "    # Convert validation_losses to CPU and detach before plotting\n",
    "    validation_losses = [v.cpu().detach().numpy() for v in validation_losses]\n",
    "\n",
    "    # Set the figure size and style\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot training loss with markers\n",
    "    plt.plot(train_losses, label='Training Loss', color='tab:blue', marker='o', markersize=6, linestyle='-', linewidth=2)\n",
    "\n",
    "    # Plot validation loss with markers and different style\n",
    "    plt.plot(validation_losses, label='Validation Loss', color='tab:orange', marker='s', markersize=6, linestyle='--', linewidth=2)\n",
    "\n",
    "    # Add labels, title, and legend with improved styles\n",
    "    plt.xlabel('Epochs', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Loss', fontsize=14, fontweight='bold')\n",
    "    plt.title('Training and Validation Loss Over Epochs', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Display the legend with adjusted positioning\n",
    "    plt.legend(loc='upper right', fontsize=12)\n",
    "\n",
    "    # Adjust x and y ticks for better readability\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_losses(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o6WTtyoNxZ2G"
   },
   "outputs": [],
   "source": [
    "# Evaluation loop\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for ts_values, ts_indicators, ts_time, static, labels in val_loader:\n",
    "        ts_values,ts_indicators,ts_time, static, labels = ts_values.to(device), ts_indicators.to(device),ts_time.to(device), static.to(device), labels.to(device)\n",
    "        outputs = model(ts_values, ts_indicators,ts_time, static)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.long()).sum().item()\n",
    "\n",
    "    print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "21P4H7sdxbFH"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the given data loader and calculates evaluation metrics.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Trained model.\n",
    "        data_loader (torch.utils.data.DataLoader): Data loader for validation/test set.\n",
    "        device (torch.device): Device to perform computation on (CPU/GPU).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing evaluation metrics.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_prob = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for ts_values, ts_indicators, ts_time, static, labels in data_loader:\n",
    "            # Move data to device\n",
    "            ts_values,ts_indicators, ts_time, static, labels = ts_values.to(device), ts_indicators.to(device), ts_time.to(device), static.to(device), labels.to(device)\n",
    "\n",
    "            # Get model predictions\n",
    "            outputs = model(ts_values, ts_indicators, ts_time, static)  # Raw logits\n",
    "            probabilities = torch.softmax(outputs, dim=1)[:, 1]  # Probability for class 1\n",
    "            predictions = torch.argmax(outputs, dim=1)  # Predicted class labels\n",
    "\n",
    "            # Collect predictions and ground truth\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predictions.cpu().numpy())\n",
    "            y_prob.extend(probabilities.cpu().numpy())\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=1)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=1)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=1)\n",
    "    roc_auc = roc_auc_score(y_true, y_prob)\n",
    "\n",
    "    return {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1,\n",
    "        \"ROC-AUC\": roc_auc,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xruKPV_1_d7N"
   },
   "source": [
    "Evaluate performance on all splits\n",
    "- Save data in the same format as baseline models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R5OEqB8GIjGQ"
   },
   "outputs": [],
   "source": [
    "import time as time\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, num_epochs = 100):\n",
    "  model.to(device)\n",
    "\n",
    "  # Loss and optimizer\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "  # Training loop\n",
    "  train_losses = []\n",
    "  val_losses = []\n",
    "  AUC_scores = []\n",
    "  times = []\n",
    "\n",
    "  for epoch in range(num_epochs):  # Adjust epochs as needed\n",
    "      start_time = time.time()\n",
    "      model.train()\n",
    "      loss_train = 0\n",
    "      for ts_values, ts_indicators, ts_time, static, labels in train_loader:\n",
    "          ts_values,ts_indicators, ts_time , static, labels = ts_values.to(device), ts_indicators.to(device), ts_time.to(device), static.to(device), labels.to(device)\n",
    "\n",
    "          # Forward pass\n",
    "          outputs = model(ts_values, ts_indicators, ts_time, static)\n",
    "          loss = criterion(outputs, labels.long())\n",
    "\n",
    "          # Backward pass\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Clip gradients\n",
    "          optimizer.step()\n",
    "\n",
    "          loss_train += loss.item()\n",
    "\n",
    "      train_losses.append(loss_train/len(train_loader))\n",
    "\n",
    "      # validation loss\n",
    "      model.eval().to(device)\n",
    "      labels_list = torch.LongTensor([]).to(device)\n",
    "      predictions_list = torch.FloatTensor([]).to(device)\n",
    "      with torch.no_grad():\n",
    "          for ts_values, ts_indicators, ts_time, static, labels in val_loader:\n",
    "              ts_values,ts_indicators, ts_time, static, labels = ts_values.to(device), ts_indicators.to(device), ts_time.to(device), static.to(device), labels.to(device).long()\n",
    "              labels_list = torch.cat((labels_list, labels), dim=0)\n",
    "              predicition = model(ts_values, ts_indicators, ts_time, static)\n",
    "              predictions_list = torch.cat((predictions_list, predicition), dim=0)\n",
    "\n",
    "          probs = torch.nn.functional.softmax(predictions_list, dim=1)\n",
    "          auc_score = roc_auc_score(labels_list.cpu().numpy(), probs[:, 1].cpu().numpy())\n",
    "          aupr_score = average_precision_score(labels_list.cpu().numpy(), probs[:, 1].cpu().numpy())\n",
    "          accuracy = accuracy_score(labels_list.cpu().numpy(), (probs[:, 1] >= 0.5).cpu().numpy())\n",
    "          AUC_scores.append(auc_score)\n",
    "\n",
    "      val_loss = criterion(predictions_list, labels_list)\n",
    "      val_losses.append(val_loss)\n",
    "      end_time = time.time()\n",
    "      delta = end_time-start_time\n",
    "      times.append(delta)\n",
    "\n",
    "      if epoch%20==0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss_train/len(train_loader):.4f}, Val Loss: {val_loss:.4f}, AUC: {auc_score:.4f}, AUPR: {aupr_score:.4f}, Accuracy: {accuracy:.4f}, Time: {delta}\")\n",
    "\n",
    "  return model, train_losses, val_losses, AUC_scores, times\n",
    "\n",
    "def test(model, test_loader):\n",
    "    model.eval().to(device)  # Set model to evaluation mode\n",
    "\n",
    "    # Loss and metrics\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    test_losses = []\n",
    "    labels_list = torch.LongTensor([]).to(device)\n",
    "    predictions_list = torch.FloatTensor([]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss_test = 0\n",
    "        for ts_values, ts_indicators, ts_time, static, labels in test_loader:\n",
    "            # Move data to device\n",
    "            ts_values, ts_indicators, ts_time, static, labels = (\n",
    "                ts_values.to(device),\n",
    "                ts_indicators.to(device),\n",
    "                ts_time.to(device),\n",
    "                static.to(device),\n",
    "                labels.to(device).long(),\n",
    "            )\n",
    "\n",
    "            # Forward pass\n",
    "            predictions = model(ts_values, ts_indicators, ts_time, static)\n",
    "            loss = criterion(predictions, labels)\n",
    "\n",
    "            # Accumulate test loss\n",
    "            loss_test += loss.item()\n",
    "\n",
    "            # Collect labels and predictions for metrics\n",
    "            labels_list = torch.cat((labels_list, labels), dim=0)\n",
    "            predictions_list = torch.cat((predictions_list, predictions), dim=0)\n",
    "\n",
    "        # Compute average test loss\n",
    "        test_losses.append(loss_test / len(test_loader))\n",
    "\n",
    "        # Compute probabilities for metrics\n",
    "        probs = torch.nn.functional.softmax(predictions_list, dim=1)\n",
    "        auc_score = roc_auc_score(labels_list.cpu().numpy(), probs[:, 1].cpu().numpy())\n",
    "        aupr_score = average_precision_score(labels_list.cpu().numpy(), probs[:, 1].cpu().numpy())\n",
    "        predicted_labels = (probs[:, 1] >= 0.5).cpu().numpy().astype(int)\n",
    "        accuracy = accuracy_score(labels_list.cpu().numpy(), predicted_labels)\n",
    "\n",
    "    # Print test results\n",
    "    print(f\"Test Loss: {test_losses[-1]:.4f}, AUC: {auc_score:.4f}, AUPR: {aupr_score:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    return test_losses, auc_score, aupr_score, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u8Z_uymw9s0x"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# empty dataframe for each split\n",
    "training_log = pd.DataFrame(columns=[\"epoch\",\t\"train_loss\",\t\"val_loss\",\t\"auc_score\", \"time\"])\n",
    "test_results = {\n",
    "    \"test_loss\": 0,\n",
    "    \"accuracy\": 0,\n",
    "    \"AUPRC\": 0,\n",
    "    \"AUROC\": 0,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "splits = range(1, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvc3Kp01Nir-"
   },
   "source": [
    "#### Loop over splits. Collect and save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AQ3eJsh2G1vM"
   },
   "outputs": [],
   "source": [
    "save_path = project_dir + f\"/results/SMART_M_timed/\"\n",
    "\n",
    "for split in splits:\n",
    "\n",
    "  # Load data\n",
    "  train_data = np.load(f'/content/drive/MyDrive/ssm_ehr/datasets/split_{split}/train_physionet2012_{split}.npy', allow_pickle=True)\n",
    "  test_data = np.load(f'/content/drive/MyDrive/ssm_ehr/datasets/split_{split}/test_physionet2012_{split}.npy', allow_pickle=True)\n",
    "  val_data = np.load(f'/content/drive/MyDrive/ssm_ehr/datasets/split_{split}/validation_physionet2012_{split}.npy', allow_pickle=True)\n",
    "\n",
    "  train_dataset = ICUTimeSeriesDataset(train_data)\n",
    "  val_dataset = ICUTimeSeriesDataset(val_data)\n",
    "  test_dataset = ICUTimeSeriesDataset(test_data)\n",
    "\n",
    "  # Dataloader\n",
    "  train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=custom_collate_fn)\n",
    "  val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=custom_collate_fn)\n",
    "  test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "\n",
    "  # Reinstantiate a model after each split\n",
    "\n",
    "  model = MoEMambaAttentionClassifier(\n",
    "    ts_feature_dim=37,\n",
    "    static_feature_dim=8,\n",
    "    hidden_dim=16,\n",
    "    num_classes=2\n",
    "  )\n",
    "\n",
    "\n",
    "\n",
    "  # Training loop\n",
    "  model, train_losses, val_losses, AUC_scores, times = train(model, train_loader, val_loader, num_epochs = 100)\n",
    "\n",
    "  training_log[\"epoch\"]=[i for i in range(1, 101)]\n",
    "  # Convert CUDA tensors to numpy-compatible values\n",
    "  training_log[\"epoch\"] = [i for i in range(1, 101)]\n",
    "  training_log[\"train_loss\"] = [loss.cpu().item() if torch.is_tensor(loss) else loss for loss in train_losses]\n",
    "  training_log[\"val_loss\"] = [loss.cpu().item() if torch.is_tensor(loss) else loss for loss in val_losses]\n",
    "  training_log[\"auc_score\"] = [auc.cpu().item() if torch.is_tensor(auc) else auc for auc in AUC_scores]\n",
    "  training_log[\"time\"] = [t.cpu().item() if torch.is_tensor(t) else t for t in times]\n",
    "\n",
    "\n",
    "  # Testing\n",
    "\n",
    "  test_losses, auc_score, aupr_score, accuracy = test(model, test_loader)\n",
    "  test_results[\"test_loss\"] = test_losses.cpu().item() if torch.is_tensor(test_losses) else test_losses\n",
    "  test_results[\"accuracy\"] = accuracy.cpu().item() if torch.is_tensor(accuracy) else accuracy\n",
    "  test_results[\"AUPRC\"] = aupr_score.cpu().item() if torch.is_tensor(aupr_score) else aupr_score\n",
    "  test_results[\"AUROC\"] = auc_score.cpu().item() if torch.is_tensor(auc_score) else auc_score\n",
    "\n",
    "  # Save results\n",
    "\n",
    "\n",
    "  train_fp = save_path+f\"split_{split}/training_log.csv\"\n",
    "  training_log.to_csv(train_fp, index=False)\n",
    "\n",
    "  test_fp = save_path+f\"split_{split}/test_results.json\"\n",
    "  json_results = json.dumps(test_results, indent=4)\n",
    "  with open(test_fp, 'w') as file:\n",
    "    file.write(json_results)\n",
    "\n",
    "  print(f\"Successfully saved data fromm split: {split}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
